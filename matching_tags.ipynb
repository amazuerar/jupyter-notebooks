{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import io\n",
    "from bson import ObjectId\n",
    "import wget\n",
    "import subprocess\n",
    "import shutil\n",
    "import nltk\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('obfuscated')\n",
    "db = client['obfuscated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategoriesBelowOrEqThresholds(thrsSymbols, thrsInstances):\n",
    "    hash_map = {}\n",
    "    years = [\"2015C1\",\"2016C1\",\"2017C1\",\"2018C1\"]\n",
    "    \n",
    "    for year in years:\n",
    "        collectionYear = db[str(year)]\n",
    "        mydoc = collectionYear.find({ 'methods_retrieved': True })\n",
    "        for doc in mydoc:\n",
    "            word = doc['matching_tag']\n",
    "            \n",
    "            for modified in doc['modified_c_files']:\n",
    "                for method in modified['methods_infile']:\n",
    "                    \n",
    "                    old_method_string = method['old_method'].strip()\n",
    "                    new_method_string = method['new_method'].strip()\n",
    "                    \n",
    "                    nltk_tokens_old = nltk.word_tokenize(old_method_string)\n",
    "                    nltk_tokens_new = nltk.word_tokenize(new_method_string)\n",
    "                    \n",
    "                    if len(nltk_tokens_old) <= thrsSymbols and len(nltk_tokens_new) <= thrsSymbols:\n",
    "\n",
    "                        if word in hash_map:\n",
    "                            hash_map[word] = hash_map[word] + 1\n",
    "                        else:\n",
    "                            hash_map[word] = 1\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "    for k,v in sorted(hash_map.items(), key=lambda d_values: d_values[1], reverse=True):\n",
    "        if v < thrsInstances:\n",
    "            hash_map.pop(k)\n",
    "            #print(k, str(v))\n",
    "    return sorted(hash_map.items(), key=lambda d_values: d_values[1], reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStringQuery(my_dict):\n",
    "    m_dict = my_dict\n",
    "    str=\"\"\n",
    "    for k in m_dict:\n",
    "        str=str+'{\"matching_tag\": \"'+k[0]+'\"},'\n",
    "    return str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesWithGivenCategories(categories, thrsSymbols):\n",
    "    years = [\"2015C1\",\"2016C1\",\"2017C1\",\"2018C1\"]\n",
    "    \n",
    "    for year in years:\n",
    "        collectionYear = db[str(year)]\n",
    "        \n",
    "        myQuery = '{\"$and\":[{ \"$or\": ['+categories+'] }, {\"methods_retrieved\": True} ] }'\n",
    "        myQueryDict = literal_eval(myQuery)\n",
    "        mydoc = collectionYear.find(myQueryDict)\n",
    "        with open(\"matching_tags\"+\".txt\", 'a') as tags:\n",
    "            with open(\"vulnerable_methods.txt\", 'a') as vuln_met:\n",
    "                for doc in mydoc:\n",
    "                    for file in doc[\"modified_c_files\"]:\n",
    "                        for method in file[\"methods_infile\"]:\n",
    "                            \n",
    "                            old_method_string = method['old_method'].strip()\n",
    "                            new_method_string = method['new_method'].strip()\n",
    "                    \n",
    "                            nltk_tokens_old = nltk.word_tokenize(old_method_string)\n",
    "                            nltk_tokens_new = nltk.word_tokenize(new_method_string)\n",
    "                    \n",
    "                            if len(nltk_tokens_old) <= thrsSymbols and len(nltk_tokens_new) <= thrsSymbols:\n",
    "                                tags.write(doc[\"matching_tag\"])\n",
    "                                tags.write('\\n')\n",
    "                                vuln_met.write(method[\"old_method\"])\n",
    "                                vuln_met.write('\\n')\n",
    "    print(\"The files have been written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesWithGivenCategoriesVulnAndNoVuln(categories, thrsSymbols):\n",
    "    years = [\"2015C1\",\"2016C1\",\"2017C1\",\"2018C1\"]\n",
    "    \n",
    "    for year in years:\n",
    "        collectionYear = db[str(year)]\n",
    "        \n",
    "        myQuery = '{\"$and\":[{ \"$or\": ['+categories+'] }, {\"methods_retrieved\": True} ] }'\n",
    "        myQueryDict = literal_eval(myQuery)\n",
    "        mydoc = collectionYear.find(myQueryDict)\n",
    "        with open(\"matching_tags_VNV\"+\".txt\", 'a') as tags:\n",
    "            with open(\"vulnerable_methods_VNV.txt\", 'a') as vuln_met:\n",
    "                for doc in mydoc:\n",
    "                    for file in doc[\"modified_c_files\"]:\n",
    "                        for method in file[\"methods_infile\"]:\n",
    "                            \n",
    "                            old_method_string = method['old_method'].strip()\n",
    "                            new_method_string = method['new_method'].strip()\n",
    "                    \n",
    "                            nltk_tokens_old = nltk.word_tokenize(old_method_string)\n",
    "                            nltk_tokens_new = nltk.word_tokenize(new_method_string)\n",
    "                    \n",
    "                            if len(nltk_tokens_old) <= thrsSymbols and len(nltk_tokens_new) <= thrsSymbols:\n",
    "                                tags.write(\"VULN\")\n",
    "                                tags.write('\\n')\n",
    "                                tags.write(\"NOVULN\")\n",
    "                                tags.write('\\n')\n",
    "                                vuln_met.write(method[\"old_method\"])\n",
    "                                vuln_met.write('\\n')\n",
    "                                vuln_met.write(method[\"new_method\"])\n",
    "                                vuln_met.write('\\n')\n",
    "    print(\"The files have been written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesByCategory(my_dict, thrsSymbols):\n",
    "    \n",
    "    years = [\"2015C1\",\"2016C1\",\"2017C1\",\"2018C1\"]\n",
    "    \n",
    "    \n",
    "    hash_map = {}\n",
    "    \n",
    "    for k in my_dict:\n",
    "        methods = []\n",
    "        category = k[0]\n",
    "        print(category)\n",
    "        current_directory = os.getcwd()\n",
    "        final_directory = os.path.join(current_directory, category)\n",
    "        if not os.path.exists(final_directory):\n",
    "            os.makedirs(final_directory)\n",
    "          \n",
    "        for year in years:\n",
    "            print(year)\n",
    "            collectionYear = db[str(year)]\n",
    "\n",
    "            myQuery = '{\"$and\":[{ \"matching_tag\": \"'+category+'\" }, {\"methods_retrieved\": True} ] }'\n",
    "            myQueryDict = literal_eval(myQuery)\n",
    "            mydoc = collectionYear.find(myQueryDict)\n",
    "            \n",
    "            with open(category+\"/matching_tags_\"+category+\".txt\", 'a') as tags:\n",
    "                with open(category+\"/vulnerable_methods_\"+category+\".txt\", 'a') as vuln_met:\n",
    "                    for doc in mydoc:\n",
    "                        word = doc['matching_tag']\n",
    "                        for file in doc[\"modified_c_files\"]:\n",
    "                            for method in file[\"methods_infile\"]:\n",
    "\n",
    "                                old_method_string = method['old_method'].strip()\n",
    "                                new_method_string = method['new_method'].strip()\n",
    "                    \n",
    "                                nltk_tokens_old = nltk.word_tokenize(old_method_string)\n",
    "                                nltk_tokens_new = nltk.word_tokenize(new_method_string)\n",
    "\n",
    "                                if len(nltk_tokens_old) <= thrsSymbols and len(nltk_tokens_new) <= thrsSymbols:\n",
    "                                    \n",
    "                                    if old_method_string == new_method_string:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        if not old_method_string in methods and not new_method_string in methods:\n",
    "                                            methods.append(old_method_string)\n",
    "                                            methods.append(new_method_string)\n",
    "                                            tags.write(\"VULN\")\n",
    "                                            tags.write('\\n')\n",
    "                                            tags.write(\"NOVULN\")\n",
    "                                            tags.write('\\n')\n",
    "                                            vuln_met.write(old_method_string)\n",
    "                                            vuln_met.write('\\n')\n",
    "                                            vuln_met.write(new_method_string)\n",
    "                                            vuln_met.write('\\n')\n",
    "                                            if word in hash_map:\n",
    "                                                hash_map[word] = hash_map[word] + 1\n",
    "                                            else:\n",
    "                                                hash_map[word] = 1\n",
    "                                        else:\n",
    "                                            continue\n",
    "                                            \n",
    "        print(\"Numero total: \"+str(len(methods)))\n",
    "        print(\"Numero unicos: \"+str(len(list(dict.fromkeys(methods)))))\n",
    "    \n",
    "    print(\"The files have been written\")\n",
    "    print(hash_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrsSymbols = 500\n",
    "thrsInstances = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_categories = getCategoriesBelowOrEqThresholds(thrsSymbols, thrsInstances)\n",
    "print(list_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_categories = getStringQuery(list_categories)\n",
    "print(string_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFilesByCategory(list_categories, thrsSymbols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
